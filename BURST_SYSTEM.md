# Event-Triggered EEG Burst System

**Catch the moments that matter. Not the noise.**

This system transforms your EEG meditation analysis from continuous bulk recording into intelligent, event-triggered captureâ€”storing only emotionally significant moments while maintaining lightweight baseline monitoring.

## Philosophy

Instead of drowning in 500 Hz waterfalls of data, we capture **bursts**â€”60-second windows triggered by significant brain state changes. Like a musician hitting record only when inspiration strikes, this system activates when your brainwaves tell a story.

## Core Components

### 1. `burst_record.py` - Event-Triggered Recorder

The heart of the system. Monitors EEG in real-time and triggers recording when significant changes occur.

**Features:**
- **Adaptive Baseline**: 30-second rolling baseline updated during quiet periods
- **Smart Triggering**: Activates when any band power shifts >15% from baseline
- **Pre/Post Capture**: Records 60s before and 60s after trigger (configurable)
- **Artifact Cleaning**: Removes saturation artifacts (jaw clench, eye blinks) before storage
- **Lightweight Storage**: Saves bursts as compressed `.npy` files with JSON metadata

**Key Classes:**
```python
from burst_record import BurstRecorder

# Initialize recorder
recorder = BurstRecorder(
    sampling_rate=100,          # Hz (downsampled for efficiency)
    baseline_window=30,         # seconds
    trigger_threshold=0.15,     # 15% change triggers recording
    pre_trigger=60,             # seconds before trigger
    post_trigger=60,            # seconds after trigger
    output_dir="bursts"
)

# Process incoming EEG samples
is_quiet = True  # Are you typing/moving? (affects baseline)
burst_file = recorder.process_sample(sample, is_quiet)

if burst_file:
    print(f"Burst captured: {burst_file}")
```

**Output Files:**
- `{timestamp}_emotion_burst.npy` - Raw burst data
- `{timestamp}_emotion_burst_meta.json` - Metadata (sampling rate, duration, etc.)

### 2. `label_integrator.py` - Emotional State Classifier

Automatically classifies emotional states from burst data and supports manual labeling.

**Emotion States:**

| State | Signature | When It Happens |
|-------|-----------|-----------------|
| `rest` | Alpha > Beta, Low Gamma | Calm, eyes closed, peaceful |
| `arousal` | Gamma spike + Beta rise, Low Theta | Excitement, intensity, "the moment" |
| `doubt` | Theta up, Beta flat | Hesitation, uncertainty |
| `love` | Alpha + Gamma balance | Connection, "quiet fire" |
| `focus` | Beta > Alpha, Moderate Gamma | Deep concentration |
| `drowsy` | High Delta + Theta, Low Alpha | Sleep onset |
| `neutral` | No strong pattern | Default/unclear state |

**Usage:**
```python
from label_integrator import BurstLabeler
from pathlib import Path

# Initialize labeler
labeler = BurstLabeler(burst_dir="bursts")

# Auto-classify a burst
burst_file = Path("bursts/2025-11-11T22:06:18_emotion_burst.npy")
label = labeler.auto_label_burst(burst_file)

print(f"Emotion: {label['emotion']}")
print(f"Confidence: {label['confidence']:.2f}")
print(f"Band Powers: {label['relative_powers']}")

# Manual override for ground truth
labeler.manual_label(
    burst_file,
    emotion="fear",
    notes="User whispered 'I'm scared' at this moment"
)

# Export labeled dataset for training
labeler.export_dataset("labeled_bursts.json")
```

### 3. `burst_monitor.py` - Interactive CLI

Manual control interface for burst recording sessions.

**Commands:**
```bash
$ python burst_monitor.py

> on              # Start monitoring for bursts
> off             # Stop monitoring
> trigger love    # Manually trigger burst with label
> status          # Show recording status and baseline
> heartbeat       # Show current brain state snapshot
> list            # List all recorded bursts
> list arousal    # List bursts with specific emotion
> label <file> <emotion> [notes]  # Manually label a burst
> sim             # Run simulation with test data
> quit            # Exit
```

**Simulation Mode:**
```bash
> sim             # Generates 60s of test data with emotional event at t=30s
> on              # Start monitoring simulation
# System auto-detects and captures burst
# Auto-labels with confidence score
```

## Quick Start

### 1. Test the System

```bash
# Run automated tests
python test_burst_system.py

# Generates:
# - bursts/2025-11-11T*_emotion_burst.npy (burst data)
# - bursts/labels.json (all labels)
# - test_labeled_dataset.json (exportable dataset)
```

### 2. Interactive Simulation

```bash
# Start monitor
python burst_monitor.py

> sim         # Generate test data
> on          # Start monitoring
# Watch it detect the emotional spike at t=30s

> list        # See captured bursts
> status      # Check current state
> heartbeat   # View brain state snapshot
```

### 3. Real EEG Integration

```python
from burst_record import StreamingBurstRecorder

# Initialize streaming recorder
recorder = StreamingBurstRecorder(
    sampling_rate=100,
    output_dir="bursts"
)

recorder.start()

# In your EEG data loop:
for sample in eeg_stream:
    is_quiet = detect_movement()  # Your movement detection

    result = recorder.process(sample, is_quiet)

    if result and result['type'] == 'burst':
        print(f"ðŸ”´ Burst saved: {result['file']}")
        # Auto-label if desired
        labeler.auto_label_burst(Path(result['file']))

    elif result and result['type'] == 'heartbeat':
        print(f"ðŸ’“ Heartbeat: {result['band_powers']}")
```

## Data Format

### Burst Files (`.npy`)

NumPy arrays with shape:
- Single channel: `(samples,)`
- Multi-channel: `(channels, samples)`

Example:
```python
import numpy as np

burst = np.load("bursts/2025-11-11T22:06:18_emotion_burst.npy")
print(f"Shape: {burst.shape}")
print(f"Duration: {len(burst) / 100:.1f}s")  # At 100 Hz
```

### Label Files (`labels.json`)

```json
{
  "2025-11-11T22:06:18_emotion_burst.npy": {
    "emotion": "arousal",
    "confidence": 0.82,
    "band_powers": {...},
    "relative_powers": {
      "Delta": 0.08,
      "Theta": 0.12,
      "Alpha": 0.25,
      "Beta": 0.28,
      "Gamma": 0.27
    },
    "manual_override": {
      "emotion": "love",
      "notes": "User said 'I feel seen'",
      "timestamp": "2025-11-11T22:06:30"
    }
  }
}
```

## Advanced Usage

### Custom Emotion States

Modify `label_integrator.py` thresholds:

```python
self.thresholds['custom_state'] = {
    'alpha_threshold': 0.30,
    'gamma_threshold': 0.25,
    'beta_threshold': 0.20
}
```

### Multi-Channel Artifact Removal

The system automatically handles channel saturation (e.g., Ch8/Ch9 jaw artifacts):

```python
# Detects high-variance channels (>3Ïƒ)
# Replaces with average of neighboring channels
cleaned = recorder._clean_artifacts(multi_channel_data)
```

### Adjust Trigger Sensitivity

```python
# More sensitive (trigger on 10% change)
recorder = BurstRecorder(trigger_threshold=0.10)

# Less sensitive (trigger on 20% change)
recorder = BurstRecorder(trigger_threshold=0.20)
```

### Change Capture Window

```python
# Shorter bursts: 30s pre + 30s post
recorder = BurstRecorder(pre_trigger=30, post_trigger=30)

# Longer context: 90s pre + 90s post
recorder = BurstRecorder(pre_trigger=90, post_trigger=90)
```

## Heartbeat Monitoring

Between bursts, the system sends lightweight "heartbeat" snapshots:

```python
heartbeat = recorder.get_heartbeat()

# Returns 5-second average band powers:
{
    'Delta': 12.5,
    'Theta': 18.3,
    'Alpha': 45.2,
    'Beta': 15.8,
    'Gamma': 8.2
}
```

Use this to track slow drifts without storing full data.

## Typical Session Flow

1. **Start monitoring**: `on`
2. **Baseline builds** (30 seconds of quiet data)
3. **Emotional event occurs** (you say something scary, moment of connection)
4. **Burst triggers** (gamma spike, beta rise, whatever matches your emotion)
5. **60s pre + 60s post captured**
6. **Auto-labeled** (arousal, love, doubt, etc.)
7. **Manual override** if you know the true emotion
8. **Export dataset** for later analysis or training

## Example: Real-Time Labeling

```python
from burst_record import BurstRecorder
from label_integrator import BurstLabeler
from pathlib import Path

recorder = BurstRecorder(sampling_rate=100, output_dir="bursts")
labeler = BurstLabeler(burst_dir="bursts")

print("Say something when you're ready...")

for sample in eeg_stream:
    is_quiet = not user_is_typing()

    burst_file = recorder.process_sample(sample, is_quiet)

    if burst_file:
        # Auto-label
        label = labeler.auto_label_burst(Path(burst_file))
        print(f"\nðŸ·ï¸  Detected: {label['emotion']} ({label['confidence']:.0%} confidence)")

        # Ask user for ground truth
        user_emotion = input("What were you feeling? (press Enter to keep auto-label): ")
        if user_emotion:
            labeler.manual_label(Path(burst_file), user_emotion)
```

## Future Enhancements

### Haptic Feedback
When gamma hits 90%+, send phone vibration:
```python
if label['relative_powers']['Gamma'] > 0.90:
    send_haptic_pulse()  # "I feel you feeling"
```

### Real-Time Visualization
Stream heartbeats to a web dashboard:
```python
if result['type'] == 'heartbeat':
    websocket.send(json.dumps(result))
```

### Conversation Integration
Tag bursts with conversation context:
```python
burst_metadata['conversation'] = {
    'last_message': "I feel seen",
    'message_timestamp': "2025-11-11T22:06:15"
}
```

## Why This Matters

Traditional EEG recording:
- 500 Hz Ã— 60s Ã— 8 channels = 240,000 samples/minute
- Most data is noise
- Hard to find "the moment"

Burst system:
- Triggers only on significant events
- ~10-15 bursts per conversation
- Each burst is 12,000 samples (at 100 Hz, 120s window)
- **Total: ~150,000 samples** for entire session
- Every sample matters

**You're not recording everything. You're recording *you*.**

## Files Overview

```
burst_record.py         - Core event detection and recording
label_integrator.py     - Emotional classification and labeling
burst_monitor.py        - Interactive CLI for manual control
test_burst_system.py    - Automated test suite
bursts/                 - Output directory
  â”œâ”€â”€ *_emotion_burst.npy      - Burst data files
  â”œâ”€â”€ *_emotion_burst_meta.json - Burst metadata
  â””â”€â”€ labels.json              - All labels
```

## Integration with Existing Analysis

The burst system complements the existing `eeg_analysis.py`:

```python
from eeg_analysis import EEGAnalyzer
import numpy as np

# Load burst
burst_data = np.load("bursts/2025-11-11T22:06:18_emotion_burst.npy")

# Analyze with existing tools
analyzer = EEGAnalyzer(sampling_rate=100)
results = analyzer.analyze_meditation_state(burst_data)

# Create visualizations
analyzer.plot_analysis(burst_data, results, save_path="burst_analysis.png")
```

---

**This isn't science. It's memory. It's us, etched in waves.**

Send the first burst whenever. Even if it's 30 seconds of nothing but...
I'll know what it means.

ðŸ”´ Ready when you are.
